{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "24-02-2021-claheTestInceptionV3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOISc+wdMK9f9B8orUN125U",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedtal/PFE2021/blob/main/24_02_2021_claheTestInceptionV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baNPHT-Men05"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time as t\n",
        "import cv2\n",
        "import warnings\n",
        "import shutil\n",
        "import os\n",
        "#import keras\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras import regularizers\n",
        "from keras.layers import Dense, Dropout\n",
        "from matplotlib import pyplot\n",
        "\n",
        "\n",
        "# Sklearn utils.\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Keras.\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications import DenseNet121\n",
        "from keras.utils import Sequence\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, ZeroPadding2D, GlobalAveragePooling2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras import regularizers\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras import optimizers\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "# to generate data augmentation a partir de aptos 2019 preprocessed\n",
        "import imageio\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib\n",
        "import os\n",
        "import cv2\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J43LjEzPBEkk",
        "outputId": "c8e48335-35e2-47bf-82e4-af92a713b687"
      },
      "source": [
        "# before all don't forget to use the GPU\n",
        "\n",
        "# at first we need to get the dataset from the drive \n",
        "# pfe2021.1@gmail.com\t\tzakimoha123\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL1HQdpZB7mN"
      },
      "source": [
        "#unzip the dataset\n",
        "\n",
        "# for the train\n",
        "!unzip /content/drive/MyDrive/dataset/DR-dataset2015.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMv-xH29qi3m"
      },
      "source": [
        "# for the test\n",
        "!unzip /content/drive/MyDrive/dataset/DR-2019-test.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k1ot8cjek_W"
      },
      "source": [
        "Clahe function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSQEAEv3eiKW"
      },
      "source": [
        "def preprocessFunction(image):\n",
        "    #image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "    kopya = image.copy()\n",
        "    kopya = cv2.cvtColor(kopya,cv2.COLOR_RGB2GRAY)\n",
        "    blur = cv2.GaussianBlur(kopya,(5,5),0)\n",
        "    thresh = cv2.threshold(blur,10,255,cv2.THRESH_BINARY)[1]\n",
        "    kontur = cv2.findContours(thresh.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    kontur = kontur[0][0]\n",
        "    kontur = kontur[:,0,:]\n",
        "    x1 = tuple(kontur[kontur[:,0].argmin()])[0]\n",
        "    x2 = tuple(kontur[kontur[:,0].argmax()])[0]\n",
        "    y1 = tuple(kontur[kontur[:,1].argmin()])[1]\n",
        "    y2 = tuple(kontur[kontur[:,1].argmax()])[1]\n",
        "    x = int(x2-x1)*4//50\n",
        "    y = int(y2-y1)*5//50\n",
        "    kopya2 = image.copy()\n",
        "    if x2-x1>100 and y2-y1>100 :\n",
        "        kopya2 = kopya2[y1+y:y2-y , x1+x:x2-x]\n",
        "        kopya2 = cv2.resize(kopya2,(400,400))\n",
        "    lab = cv2.cvtColor(kopya2,cv2.COLOR_RGB2LAB)\n",
        "    l,a,b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=5.0,tileGridSize=((8,8)))\n",
        "    cl = clahe.apply(l)\n",
        "    limg = cv2.merge((cl,a,b))\n",
        "    son = cv2.cvtColor(limg,cv2.COLOR_LAB2RGB)\n",
        "    med_son = cv2.medianBlur(son,3)\n",
        "    arka_plan = cv2.medianBlur(son,37)\n",
        "    maske = cv2.addWeighted(med_son,1,arka_plan,-1,255)\n",
        "    son_img = cv2.bitwise_and(maske,med_son)\n",
        "    return son_img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GTEDUb3CSHy"
      },
      "source": [
        "# we need to delete samples from class 0 ( there is 25000 images, we are going to take just 7000)\n",
        "import random\n",
        "import os\n",
        "BASE_DIR = \"preprocessed/2\"\n",
        "NEW_DIR= \"preprocessed/1\"\n",
        "\n",
        "list_dir =  os.listdir(BASE_DIR)\n",
        "to_val = random.sample(list_dir, len(list_dir))\n",
        "for images in to_val:\n",
        "    os.rename(BASE_DIR + \"/\"+ images, NEW_DIR + \"/\" + images )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP6L6e9P3Ldl"
      },
      "source": [
        "# to create the preprocessed data a partir de aptos 2019\n",
        "\n",
        "import random\n",
        "import os\n",
        "BASE_DIR = \"DR-dataset2019-test/test/4\"\n",
        "NEW_DIR= \"preprocessed/4\"\n",
        "\n",
        "list_dir =  os.listdir(BASE_DIR)\n",
        "\n",
        "for images in list_dir:\n",
        "    image = cv2.imread(BASE_DIR + \"/\"+ images)\n",
        "    image = cv2.resize(image,(400,400))\n",
        "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "    try:\n",
        "        cv2.imwrite(NEW_DIR + \"/\" + images,cv2.cvtColor(preprocessFunction(image),cv2.COLOR_BGR2RGB))\n",
        "    except Exception: \n",
        "        pass\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajJUZBFKqov0",
        "outputId": "8255f1e3-aedc-4b79-930d-e3917d08de6f"
      },
      "source": [
        "#!rm -r DR-dataset2019-Preprocessed/3\n",
        "!ls preprocessed/0 | wc -l\n",
        "!ls preprocessed/1 | wc -l\n",
        "!ls preprocessed/2 | wc -l\n",
        "!ls preprocessed/3 | wc -l\n",
        "!ls preprocessed/4 | wc -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1805\n",
            "1369\n",
            "488\n",
            "ls: cannot access 'preprocessed/3': No such file or directory\n",
            "0\n",
            "ls: cannot access 'preprocessed/4': No such file or directory\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIdhh1orNntp"
      },
      "source": [
        "BASE_DIR = \"preprocessed/0\"\n",
        "NEW_DIR= \"preprocessed/0\"\n",
        "\n",
        "list_dir =  os.listdir(BASE_DIR)\n",
        "from google.colab.patches import cv2_imshow\n",
        "for images in list_dir:\n",
        "    image = cv2.imread(BASE_DIR + \"/\"+ images)\n",
        "    #image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "    # rotation\n",
        "    \n",
        "    rotate=iaa.Affine(rotate=(-30, -10))\n",
        "    rotated_image=rotate.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'rotated-'+images,rotated_image)\n",
        "\n",
        "      #flip H\n",
        "    flip_hr=iaa.Fliplr(p=1.0)\n",
        "    flip_hr_image= flip_hr.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'fliph-'+images,flip_hr_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTznGWYHEDiu"
      },
      "source": [
        "BASE_DIR = \"preprocessed/1\"\n",
        "NEW_DIR= \"preprocessed/1\"\n",
        "\n",
        "list_dir =  os.listdir(BASE_DIR)\n",
        "from google.colab.patches import cv2_imshow\n",
        "for images in list_dir:\n",
        "    image = cv2.imread(BASE_DIR + \"/\"+ images)\n",
        "    #image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "    # rotation\n",
        "    \n",
        "    rotate=iaa.Affine(rotate=(10, 30))\n",
        "    rotated_image=rotate.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'rotated-'+images,rotated_image)\n",
        "\n",
        "      #flip H\n",
        "    flip_hr=iaa.Fliplr(p=1.0)\n",
        "    flip_hr_image= flip_hr.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'fliph-'+images,flip_hr_image)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2hTLSSnEOxU"
      },
      "source": [
        "BASE_DIR = \"preprocessed/2\"\n",
        "NEW_DIR= \"preprocessed/2\"\n",
        "\n",
        "list_dir =  os.listdir(BASE_DIR)\n",
        "from google.colab.patches import cv2_imshow\n",
        "for images in list_dir:\n",
        "    image = cv2.imread(BASE_DIR + \"/\"+ images)\n",
        "    #image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "        # rotation\n",
        "    \n",
        "    rotate=iaa.Affine(rotate=(0, 30))\n",
        "    rotated_image=rotate.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'rotated-'+images,rotated_image)\n",
        "\n",
        "    rotate=iaa.Affine(rotate=(-30, 0))\n",
        "    rotated_image=rotate.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'rotated1-'+images,rotated_image)\n",
        "\n",
        "    #zoom\n",
        "    crop = iaa.Crop(percent=(0.15, 0.3)) # crop image\n",
        "    corp_image=crop.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'zoomed-'+images,corp_image)\n",
        "\n",
        "    crop = iaa.Crop(percent=(0, 0.15)) # crop image\n",
        "    corp_image=crop.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'zoomed1-'+images,corp_image)\n",
        "\n",
        "    crop = iaa.Affine(shear=15)\n",
        "    corp_image=crop.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'affine-'+images,corp_image)\n",
        "\n",
        "    #flip H\n",
        "    flip_hr=iaa.Fliplr(p=1.0)\n",
        "    flip_hr_image= flip_hr.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'fliph-'+images,flip_hr_image)\n",
        "\n",
        "    #flip V\n",
        "    flip_vr=iaa.Flipud(p=1.0)\n",
        "    flip_vr_image= flip_vr.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'flipv-'+images,flip_vr_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZNtZxf7X-AA"
      },
      "source": [
        "# to generate data augmentation a partir de aptos 2019 preprocessed\n",
        "import imageio\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib\n",
        "import os\n",
        "import cv2\n",
        "%matplotlib inline\n",
        "\n",
        "BASE_DIR = \"preprocessed/4\"\n",
        "NEW_DIR= \"preprocessed/4\"\n",
        "\n",
        "list_dir =  os.listdir(BASE_DIR)\n",
        "from google.colab.patches import cv2_imshow\n",
        "for images in list_dir:\n",
        "    image = cv2.imread(BASE_DIR + \"/\"+ images)\n",
        "    #image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "    # rotation\n",
        "    \n",
        "    rotate=iaa.Affine(rotate=(0, 30))\n",
        "    rotated_image=rotate.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'rotated-'+images,rotated_image)\n",
        "\n",
        "    rotate=iaa.Affine(rotate=(-30, 0))\n",
        "    rotated_image=rotate.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'rotated1-'+images,rotated_image)\n",
        "\n",
        "    #zoom\n",
        "    crop = iaa.Crop(percent=(0.15, 0.3)) # crop image\n",
        "    corp_image=crop.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'zoomed-'+images,corp_image)\n",
        "\n",
        "    crop = iaa.Crop(percent=(0, 0.15)) # crop image\n",
        "    corp_image=crop.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'zoomed1-'+images,corp_image)\n",
        "\n",
        "    crop = iaa.Affine(shear=15)\n",
        "    corp_image=crop.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'affine-'+images,corp_image)\n",
        "\n",
        "    #flip H\n",
        "    flip_hr=iaa.Fliplr(p=1.0)\n",
        "    flip_hr_image= flip_hr.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'fliph-'+images,flip_hr_image)\n",
        "    #flip V\n",
        "    flip_vr=iaa.Flipud(p=1.0)\n",
        "    flip_vr_image= flip_vr.augment_image(image)\n",
        "    cv2.imwrite(NEW_DIR + \"/\" + 'flipv-'+images,flip_vr_image)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b87ALj01I8BT",
        "outputId": "20817b57-d335-41ca-be74-cc24ffa7f6ed"
      },
      "source": [
        "#!rm -r DR-dataset2019-Preprocessed/3\n",
        "!ls preprocessed/0 | wc -l\n",
        "!ls preprocessed/1 | wc -l\n",
        "!ls preprocessed/2 | wc -l\n",
        "!ls preprocessed/3 | wc -l\n",
        "!ls preprocessed/4 | wc -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5415\n",
            "4107\n",
            "3904\n",
            "ls: cannot access 'preprocessed/3': No such file or directory\n",
            "0\n",
            "ls: cannot access 'preprocessed/4': No such file or directory\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "4N3xW0XNCw8w",
        "outputId": "7664bf77-fc99-44b8-cc4d-e7bef9967ad3"
      },
      "source": [
        "# we use the ImageDataGenerator fro : loading data into batches, pre-processing operation, data augmentation\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# the train generator with data augmentation and preprocessing (rescale)\n",
        "# https://xzz201920.medium.com/all-you-need-to-you-about-imagedatagenerator-in-keras-tensorflow-8fd436e4c0cd\n",
        "# https://medium.com/swlh/data-augmentation-using-keras-4a852e49589f\n",
        "# https://towardsdatascience.com/image-data-generators-in-keras-7c5fc6928400\n",
        "# https://keras.io/api/preprocessing/image/\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
        "\n",
        "batchSize = 32\n",
        "\n",
        "# ******** for train dataset\n",
        "\n",
        "# with data augmentation \n",
        "# https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/\n",
        "train_datagen = ImageDataGenerator(\n",
        "    #preprocessing_function = preprocessFunction,\n",
        "    rescale=1./255,\n",
        "    validation_split=0.15,) \n",
        "\n",
        "# without data augmentation\n",
        "'''train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.15)'''\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'preprocessed/',\n",
        "    classes = ['0','1','2'],\n",
        "    target_size=(400, 400),\n",
        "    color_mode='rgb',\n",
        "    batch_size=batchSize,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True)\n",
        "\n",
        "# ******* for validation dataset\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    'preprocessed/',\n",
        "    classes = ['0','1','2'],\n",
        "    class_mode='categorical',\n",
        "    target_size=(400, 400),\n",
        "    color_mode='rgb',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# ******** for test dataset\n",
        "'''\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    'DR-dataset2019-test/test/',\n",
        "    classes = ['0','1','2','3','4'],\n",
        "    target_size=(400, 400),\n",
        "    class_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    batch_size = batchSize,\n",
        "    shuffle = False)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 11413 images belonging to 3 classes.\n",
            "Found 2013 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ntest_datagen = ImageDataGenerator(rescale=1./255)\\ntest_generator = test_datagen.flow_from_directory(\\n    'DR-dataset2019-test/test/',\\n    classes = ['0','1','2','3','4'],\\n    target_size=(400, 400),\\n    class_mode='categorical',\\n    color_mode='rgb',\\n    batch_size = batchSize,\\n    shuffle = False)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY0Gjypm1w8-"
      },
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.python.keras import Model\n",
        "from tensorflow.python.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "pre_trained_model = InceptionV3(input_shape = (400,400,3), # Shape of our images\n",
        "                                include_top = False, # Leave out the last fully connected layer\n",
        "                                weights = 'imagenet')\n",
        "\n",
        "\n",
        "####\n",
        "model = Sequential()\n",
        "model.add(pre_trained_model)\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=optimizers.Adam(lr=0.00005),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "print(model.summary())\n",
        "####\n",
        "####\n",
        "\"\"\"\n",
        "x = layers.Flatten(input_shape=pre_trained_model.output.shape)(pre_trained_model.output)\n",
        "x = Dense(1024, activation='sigmoid')(x)\n",
        "predictions = Dense(num_classes, activation='softmax', name='pred')(x)\n",
        "model = Model(inputs=[pre_trained_model.input], outputs=[predictions])\n",
        "\n",
        "# compile the model and specify hyperparameters\n",
        "# https://medium.com/ml-cheat-sheet/winning-at-loss-functions-common-loss-functions-that-you-should-know-a72c1802ecb4\n",
        "# https://neptune.ai/blog/keras-loss-functions\n",
        "lossFunction = 'categorical_crossentropy'\n",
        "optimizeR = 'adam'\n",
        "model.compile(loss = lossFunction, optimizer=optimizeR, metrics=['accuracy'])\n",
        "\"\"\"\n",
        "####\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPWM85RLqBAj",
        "outputId": "72e43858-75b6-410c-c50e-08b8a0472293"
      },
      "source": [
        "def build_vgg16(num_classes, img_size):\n",
        "    from tensorflow.keras.applications import VGG16\n",
        "    from tensorflow.python.keras import Model\n",
        "    from tensorflow.python.keras.layers import Dense, Flatten\n",
        "    vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=img_size)\n",
        "    x = Flatten(input_shape=vgg16.output.shape)(vgg16.output)\n",
        "    x = Dense(1024, activation='sigmoid')(x)\n",
        "    predictions = Dense(num_classes, activation='softmax', name='pred')(x)\n",
        "    model = Model(inputs=[vgg16.input], outputs=[predictions])\n",
        "    for layer in model.layers[:8]:\n",
        "        layer.trainable = False\n",
        "    return model\n",
        "\n",
        "model = build_vgg16(3,(400,400,3))\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=optimizers.Adam(lr=0.00005),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFpo3pg5EsGW",
        "outputId": "ceca7f3a-be7a-42fa-abab-6618f69a14de"
      },
      "source": [
        "# start the train\n",
        "import math\n",
        "import tensorflow as tf\n",
        "\n",
        "#from  tensorflow.callbacks import TensorBoard\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='./', mode='max', monitor='val_accuracy', verbose=2, save_best_only=True)\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "batchSize = 32\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=math.ceil(train_generator.samples//batchSize),\n",
        "    epochs=20,\n",
        "    validation_data = val_generator,\n",
        "    validation_steps = math.ceil(val_generator.samples//batchSize),callbacks=callbacks_list,\n",
        "    verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "356/356 [==============================] - 190s 428ms/step - loss: 0.4941 - accuracy: 0.8050 - val_loss: 0.3305 - val_accuracy: 0.8443\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.84425, saving model to ./\n",
            "INFO:tensorflow:Assets written to: ./assets\n",
            "Epoch 2/20\n",
            "356/356 [==============================] - 146s 409ms/step - loss: 0.2317 - accuracy: 0.9066 - val_loss: 0.2432 - val_accuracy: 0.8886\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.84425 to 0.88861, saving model to ./\n",
            "INFO:tensorflow:Assets written to: ./assets\n",
            "Epoch 3/20\n",
            "356/356 [==============================] - 146s 409ms/step - loss: 0.1746 - accuracy: 0.9305 - val_loss: 0.2533 - val_accuracy: 0.8826\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88861\n",
            "Epoch 4/20\n",
            "356/356 [==============================] - 145s 408ms/step - loss: 0.1210 - accuracy: 0.9543 - val_loss: 0.2178 - val_accuracy: 0.9118\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.88861 to 0.91179, saving model to ./\n",
            "INFO:tensorflow:Assets written to: ./assets\n",
            "Epoch 5/20\n",
            "356/356 [==============================] - 146s 408ms/step - loss: 0.0847 - accuracy: 0.9702 - val_loss: 0.2217 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.91179\n",
            "Epoch 6/20\n",
            "356/356 [==============================] - 145s 408ms/step - loss: 0.0802 - accuracy: 0.9718 - val_loss: 0.2453 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.91179\n",
            "Epoch 7/20\n",
            "356/356 [==============================] - 145s 408ms/step - loss: 0.0454 - accuracy: 0.9847 - val_loss: 0.2410 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.91179\n",
            "Epoch 8/20\n",
            "356/356 [==============================] - 145s 408ms/step - loss: 0.0440 - accuracy: 0.9873 - val_loss: 0.2443 - val_accuracy: 0.9229\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.91179 to 0.92288, saving model to ./\n",
            "INFO:tensorflow:Assets written to: ./assets\n",
            "Epoch 9/20\n",
            "356/356 [==============================] - 145s 408ms/step - loss: 0.0313 - accuracy: 0.9901 - val_loss: 0.3716 - val_accuracy: 0.8957\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.92288\n",
            "Epoch 10/20\n",
            "356/356 [==============================] - 145s 408ms/step - loss: 0.0436 - accuracy: 0.9841 - val_loss: 0.2152 - val_accuracy: 0.9239\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.92288 to 0.92389, saving model to ./\n",
            "INFO:tensorflow:Assets written to: ./assets\n",
            "Epoch 11/20\n",
            "356/356 [==============================] - 145s 408ms/step - loss: 0.0227 - accuracy: 0.9926 - val_loss: 0.1915 - val_accuracy: 0.9410\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.92389 to 0.94103, saving model to ./\n",
            "INFO:tensorflow:Assets written to: ./assets\n",
            "Epoch 12/20\n",
            "356/356 [==============================] - 145s 408ms/step - loss: 0.0163 - accuracy: 0.9943 - val_loss: 0.2931 - val_accuracy: 0.9244\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.94103\n",
            "Epoch 13/20\n",
            "356/356 [==============================] - 145s 407ms/step - loss: 0.0234 - accuracy: 0.9921 - val_loss: 0.3117 - val_accuracy: 0.8911\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.94103\n",
            "Epoch 14/20\n",
            "356/356 [==============================] - 145s 407ms/step - loss: 0.0451 - accuracy: 0.9840 - val_loss: 0.2368 - val_accuracy: 0.9168\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.94103\n",
            "Epoch 15/20\n",
            "356/356 [==============================] - 145s 408ms/step - loss: 0.0146 - accuracy: 0.9939 - val_loss: 0.2809 - val_accuracy: 0.9194\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.94103\n",
            "Epoch 16/20\n",
            "356/356 [==============================] - 145s 408ms/step - loss: 0.0126 - accuracy: 0.9947 - val_loss: 0.2380 - val_accuracy: 0.9345\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.94103\n",
            "Epoch 17/20\n",
            "356/356 [==============================] - 145s 408ms/step - loss: 0.0265 - accuracy: 0.9892 - val_loss: 0.2372 - val_accuracy: 0.9345\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.94103\n",
            "Epoch 18/20\n",
            "356/356 [==============================] - 145s 408ms/step - loss: 0.0107 - accuracy: 0.9951 - val_loss: 0.3109 - val_accuracy: 0.9153\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.94103\n",
            "Epoch 19/20\n",
            "356/356 [==============================] - 145s 408ms/step - loss: 0.0174 - accuracy: 0.9927 - val_loss: 0.2872 - val_accuracy: 0.9289\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.94103\n",
            "Epoch 20/20\n",
            "356/356 [==============================] - 145s 408ms/step - loss: 0.0060 - accuracy: 0.9970 - val_loss: 0.2425 - val_accuracy: 0.9385\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.94103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIOLqaXCbNGS"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRowd_cb4Wyn"
      },
      "source": [
        "# saving the model\n",
        "model.save(\"DR-InceptionV3Aptos3Class.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjBZHydW4hkt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "8b4e658a-f359-4521-b230-fe626ef23320"
      },
      "source": [
        "# display the performance of the model graphically\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "print(history.history.keys())\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "#plt.plot(epochs, loss, 'g', label='Training loss')\n",
        "#plt.plot(epochs, val_loss, 'y', label='Validation loss')\n",
        "plt.title('Training and validation')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98WqjORlIeMX",
        "outputId": "cab82626-6e16-4b70-e928-305a4c9e083a"
      },
      "source": [
        "score = model.evaluate(val_generator)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 19s 297ms/step - loss: 0.2441 - accuracy: 0.9379\n",
            "[0.24411308765411377, 0.937903642654419]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMNzsbJsIzNF",
        "outputId": "6cfe4d5c-3900-40c4-99d1-06571f889977"
      },
      "source": [
        "import numpy as np\n",
        "probabilities = model.predict_generator(val_generator)\n",
        "y_true = val_generator.classes\n",
        "y_pred = np.argmax(probabilities, axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULRblMJTIR6s",
        "outputId": "89561cd2-3a32-4729-cc48-7adb7a50e7a7"
      },
      "source": [
        "print(y_pred,y_true)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 4 0 ... 2 0 2] [0 0 0 ... 4 4 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43Rx0vr6JL7S",
        "outputId": "b788f924-c555-41e6-fc66-e92de9275e3d"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "font = {\n",
        "'family': 'Times New Roman',\n",
        "'size': 12\n",
        "}\n",
        "plt.rc('font', **font)\n",
        "mat = confusion_matrix(y_true, y_pred)\n",
        "print(mat)\n",
        "class_labels = list(val_generator.class_indices.keys())\n",
        "report = classification_report(y_true, y_pred, target_names=class_labels)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[338 235 239]\n",
            " [252 201 163]\n",
            " [230 196 159]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.42      0.41       812\n",
            "           1       0.32      0.33      0.32       616\n",
            "           2       0.28      0.27      0.28       585\n",
            "\n",
            "    accuracy                           0.35      2013\n",
            "   macro avg       0.34      0.34      0.34      2013\n",
            "weighted avg       0.35      0.35      0.35      2013\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEy0jpFeG4e2"
      },
      "source": [
        "nb_row = 4\n",
        "nb_col = 6\n",
        "nb = 1\n",
        "plt.figure(figsize=(25, 15))\n",
        "for row in df.itertuples():\n",
        "    if nb > nb_col * nb_row:\n",
        "        break\n",
        "    plt.subplot(nb_row, nb_col, nb)\n",
        "    plt.imshow(cv2.cvtColor(imagePreprocessing(openImage(row), normalize=False), cv2.COLOR_BGR2RGB))\n",
        "    plt.title('Diagnosed {}'.format(row.diagnosis))\n",
        "    nb += 1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjbr26RzHCLU"
      },
      "source": [
        "!zip !zip -r preprocessed{.zip,}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}